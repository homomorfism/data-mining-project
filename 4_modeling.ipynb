{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommender System\n",
    "**Course:** Data Mining <br>\n",
    "**Authors:** Lada Morozova, Shamil Arslanov, Danis Alukaev, Maxim Faleev, Rizvan Iskaliev <br>\n",
    "**Group:** B19-DS-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Modeling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes modelling tecniques employed in this project. We have organised our development process in iterative manner, i.e. on each iteration we came up with a novel architecture that potentially outperforms the current SoA method. Recent advances in the field were taken from [\"A review on deep learning for recommender systems: challenges and remedies\"](https://link.springer.com/article/10.1007/s10462-018-9654-y) by Batmaz Z. et al. In total, there are four different implemented models: item popularity model, multilayer perceptron, factorization machine, and behaviour sequence transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Item Popularity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model grounds on assumption that most popular movies are likely to be interested for a user. We define popularity in terms of ratings number. Thus, for each user recommender system always return top-k movies with a highest number of ratings. Certainly, this is a naive approach and is not likely to satisfy our data mining goal. For this reason, we will assume that the company uses this model in its current setup, and benchmarking of all other (more sophisticated) models will be performed in comparison with performance of this \"baseline\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model is simple, the great thing about it is the lack of assumptions about the data. Essentially, we can apply this method to any dataset until it contains user ratings and names of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quite natural option is to use Multilayer Perceptron (MLP). As an input this model takes concatenated latent representation of user, movie, age, occupation, gender, as well as other manually derived on previous step features. This data is passed through multiple dense layers with non-linear activations, e.g. rectified linear unit. The output layer is a single neuron with logit, that is activated using sigmoid function. The result can be interpreted as a probability that the user will enjoy the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model uses PyTorch embeddings to encode user, movie, age, occupation and gender. Therefore, the input tensor should consist of indices that can be refered to these embeddings. Also, it is quite important that the input tensor is uniformly distributed, no missing values are allowed, and the input is numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until recently, de facto the silver bullet in the field of recommendation systems were factorization machines. They were firstly presented in [\"Factorization Machines\"](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) by Rendel S. in 2010. It was inspired by and proposed as a substitution for traditional matrix factorization. This approach allows to learn interactions between user and movies, and at the same time involve user metadata for final prediction. Thus, this approach resolves well-known problem of the [cold-start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model uses PyTorch embeddings to encode user, movie, age, occupation and gender. Therefore, the input tensor should consist of indices that can be refered to these embeddings. Also, it is quite important that the input tensor is uniformly distributed, no missing values are allowed, and the input is numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we decided to apply Behaviour Sequence Transformer (BST) architecture for our problem. The implementation is mainly inspired by manuscript [\"Behaviour Sequence Transformer for E-commerce Recommendation in Alibaba\"](https://arxiv.org/pdf/1905.06874.pdf). Long story short, authors expand MLP using sequence-to-sequence model with multi-head attention mechanism. Now the model not only takes into account metadata of user and movie, but also rating history. They claim to increase online Click-Through-Rate (CTR) gain by 7.57% compared to a control group using BST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*ThXfG04qnukM6fqWLRb_JQ.jpeg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model uses PyTorch embeddings to encode user, movie, age, occupation and gender. Therefore, the input tensor should consist of indices that can be refered to these embeddings. Also, it is quite important that the input tensor is uniformly distributed, no missing values are allowed, and the input is numeric. Moreover, the input sequence must have an upper limit, it order to feed the behaviour sequence in model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Test Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to slightly relax our problem, let's reformulate it from regression to binary classification. Recall that in original dataset grades are from 1 to 5, thus new target is 1 if the grade above 3, and 0 - otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be splitted in train, test, and validations subsets with size of approximately 70, 20, and 10% respectively. The data is splitted according to the timestamp to make sure that it inherits its historical order. Iteratively model is trained on train subset and tested on test subset (to check how model generalizes data, detect over-fitting, etc). Final performance of models is checked on validation subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankly speaking, the recommender system cannot be properly tested without external validation. Apparently, it seems that the best way is to design the A/B testing, and check how the new approach influenced certain metric, e.g. CTR. This project does not require deployment, so building infrastructure is out of the interest scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_dir = Path(\"./data/movie_lens/movie_lens_1m.csv\")\n",
    "joint_dir = Path(\"./data/augmented_movie_lens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df = pd.read_csv(movielens_dir)\n",
    "joint_df = pd.read_csv(joint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(df, columns):\n",
    "    # TODO: make assert\n",
    "    _df = df.copy()\n",
    "    offset = 0\n",
    "    for column in columns:\n",
    "        index_column = column + '_index'\n",
    "        _df[index_column] = offset + _df[column].astype('category').cat.codes\n",
    "        offset += len(_df[index_column].unique())\n",
    "    return _df\n",
    "\n",
    "columns = ['UserID', 'MovieID', 'Gender', 'Age', 'Occupation']\n",
    "movielens_df_indexed = index(movielens_df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    _df = df.copy()\n",
    "    train_df = _df[df.Timestamp <= 975e6]\n",
    "    test_df = _df[(df.Timestamp > 975e6) & (df.Timestamp < 98e7)]\n",
    "    val_df = _df[df.Timestamp >= 98e7]\n",
    "    return train_df, test_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movielens_df, test_movielens_df, val_movielens_df = split_dataset(movielens_df_indexed)\n",
    "train_joint_df, test_joint_df, val_joint_df = split_dataset(joint_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(movielens_df) == len(train_movielens_df) + len(test_movielens_df) + len(val_movielens_df)\n",
    "assert len(joint_df) == len(train_joint_df) + len(test_joint_df) + len(val_joint_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_mapping = dict(movielens_df_indexed[['Title', 'MovieID_index']].drop_duplicates().values[:, ::-1])\n",
    "gender_mapping = dict(movielens_df_indexed[['Gender', 'Gender_index']].drop_duplicates().values[:, ::-1])\n",
    "age_mapping = dict(movielens_df_indexed[['Age', 'Age_index']].drop_duplicates().values[:, ::-1])\n",
    "occupation_mapping = dict(movielens_df_indexed[['Occupation', 'Occupation_index']].drop_duplicates().values[:, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['UserID_index', 'MovieID_index', 'Age_index', 'Gender_index', 'Occupation_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_target = np.where(train_movielens_df.Rating.values < 4, 0, 1)\n",
    "X = torch.LongTensor(train_movielens_df[features].values)\n",
    "y = torch.LongTensor(binary_target).float()\n",
    "trainset = data.TensorDataset(X, y)\n",
    "\n",
    "binary_target = np.where(test_movielens_df.Rating.values < 4, 0, 1)\n",
    "X = torch.LongTensor(test_movielens_df[features].values)\n",
    "y = torch.LongTensor(binary_target).float()\n",
    "testset = data.TensorDataset(X, y)\n",
    "\n",
    "binary_target = np.where(val_movielens_df.Rating.values < 4, 0, 1)\n",
    "X = torch.LongTensor(val_movielens_df[features].values)\n",
    "y = torch.LongTensor(binary_target).float()\n",
    "valset = data.TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(trainset, batch_size=1024, shuffle=True)\n",
    "testloader = data.DataLoader(testset, batch_size=1024, shuffle=True)\n",
    "valloader = data.DataLoader(valset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = movielens_df_indexed[features].values.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metric Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of model is defined in terms of percentage of suggested movies which the user rated 4 or 5 among suggested movies which user rated (further refered as accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(ratings, recommendation):\n",
    "    \"\"\"Compute accuracy of recommender system.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ratings : pandas.DataFrame\n",
    "        dataframe with user's ratings\n",
    "    \n",
    "    recommendation : iterable \n",
    "        recommendations of a system\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        percentage of suggested movies which the user rated 4 or 5 \n",
    "        among suggested movies which user rated\n",
    "    \"\"\"\n",
    "    well_rated = ratings[ratings.Rating >= 4].MovieID_index\n",
    "    all_rated = ratings.MovieID_index\n",
    "    hits = len(list(set(well_rated) & set(recommendation)))\n",
    "    intersection = len(list(set(all_rated) & set(recommendation)))\n",
    "    if intersection == 0:\n",
    "        return np.NaN\n",
    "    accuracy = hits / intersection\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_accuracy(df, model):\n",
    "    \"\"\"Compute mean accuracy of inferenced model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        validation dataset\n",
    "\n",
    "    model : Object or torch.nn.Module\n",
    "        instance of class implementing predict method\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_accuracy : float\n",
    "        mean accuracy of recommendations\n",
    "    \"\"\"\n",
    "    accuracies = list()\n",
    "    for user_id in df.UserID.unique():\n",
    "        ratings = df[df.UserID == user_id]\n",
    "        indices = model.predict(ratings)\n",
    "        ratings = ratings[['MovieID_index', 'Rating']]\n",
    "        accuracy = compute_accuracy(ratings, indices)\n",
    "        accuracies.append(accuracy)\n",
    "    accuracies = np.array(accuracies)\n",
    "    accuracies = accuracies[~np.isnan(accuracies)]\n",
    "    mean_accuracy = accuracies.mean()\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains implementations for item popularity model, multilayer perceptron, factorization machine, and transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Item Popularity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPopularityModel:\n",
    "    \"\"\"Implementation of Item Popularity model for recommender systems.\"\"\"\n",
    "\n",
    "    def __init__(self, top_k=50):\n",
    "        \"\"\"Constructor of Item Popularity model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        top_k : int\n",
    "            number of items to recommend\n",
    "        \"\"\"\n",
    "        self.top_k = top_k\n",
    "        self.prediction = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Training routine.\n",
    "\n",
    "        Sorts movies by nubmer of ratings. Sets indices of top k movies in \n",
    "        attribute prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            data to use for training\n",
    "        \"\"\"\n",
    "        assert 'MovieID_index' in df and 'Rating' in df, '\"MovieID_index\" and \"Rating\" should be in column names'\n",
    "        _df = df.copy()\n",
    "        ratings_number = _df[['MovieID_index', 'Rating']].groupby(['MovieID_index']).count()\n",
    "        sorted_titles = ratings_number.sort_values(by='Rating', ascending=False)\n",
    "        sorted_titles.reset_index(inplace=True)\n",
    "        self.prediction = sorted_titles\n",
    "        \n",
    "    def predict(self, df=None):\n",
    "        \"\"\"Inference model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            data to use for training\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        indices : list\n",
    "            indices of recommended movies\n",
    "        \"\"\"\n",
    "        recommendation = self.prediction[:self.top_k]\n",
    "        indices = recommendation['MovieID_index']\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_popularity_model = ItemPopularityModel()\n",
    "item_popularity_model.fit(train_movielens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Implementation of Multilayer Perceptron with embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, n, layers=[600, 400, 200, 50]):\n",
    "        \"\"\"Constructor of class MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            number of entities to embed\n",
    "        \n",
    "        layers : list\n",
    "            list of layers sizes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert layers[0] % 5 == 0\n",
    "        self.embeddings = nn.Embedding(n, layers[0] // 5)\n",
    "        architecture = list()\n",
    "        for idx in range(len(layers) - 1):\n",
    "            architecture.append(nn.Linear(layers[idx], layers[idx + 1]))\n",
    "            architecture.append(nn.ReLU())\n",
    "        architecture.append(nn.Linear(layers[-1], 1))\n",
    "        architecture.append(nn.Sigmoid())\n",
    "        self.main = nn.Sequential(*architecture)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeded = self.embeddings(x)\n",
    "        x = embeded.flatten(start_dim=1)\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel: \n",
    "    \n",
    "    def __init__(self, n, trainloader, testloader,\n",
    "                 lr=1e-3, weight_decay=1e-5, epochs=10, top_k=50, device=\"cuda\"):\n",
    "        self.n = n\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.model = MLP(n).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, \n",
    "                                                        milestones=list(range(0, epochs + 1, 5))[1:], \n",
    "                                                        gamma=0.1)\n",
    "        self.criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "        \"High-level outline of the training process.\"\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_loss = self._test()\n",
    "            self.scheduler.step()\n",
    "            if not verbose:\n",
    "                print(f\"Epoch #{epoch + 1} | Train loss: {train_loss:.4f} | Test loss: {valid_loss:.4f} |\")\n",
    "        if verbose:\n",
    "            print(f\"Final train loss: {train_loss:.4f}, test loss: {valid_loss:.4f}\")\n",
    "        return self.model\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        \"Train model for one epoch on train dataset.\"\n",
    "        device = self.device\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(x.to(device)).flatten()\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.trainloader.dataset)\n",
    "    \n",
    "    def _test(self):\n",
    "        \"Test model on validation dataset.\"\n",
    "        device = self.device\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        for x, y in self.testloader:                    \n",
    "            with torch.no_grad():\n",
    "                y_hat = self.model(x.to(device)).flatten()\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            test_loss += loss.item() * x.shape[0]\n",
    "        return test_loss / len(self.testloader.dataset)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        user_id = int(x.UserID_index.unique())\n",
    "        age = int(x.Age_index.unique())\n",
    "        gender = int(x.Gender_index.unique())\n",
    "        occupation = int(x.Occupation_index.unique())\n",
    "\n",
    "        movies = movielens_df_indexed.drop_duplicates('MovieID_index').copy()\n",
    "\n",
    "        rankings = []\n",
    "        for movie in movies.MovieID_index.unique():\n",
    "            ranking = self.model(torch.LongTensor([user_id, movie, age, gender, occupation]).unsqueeze(0).to(device))\n",
    "            rankings.append(ranking.item())\n",
    "        \n",
    "        rankings = torch.FloatTensor(rankings)\n",
    "        recommendations = [i for i in movies.iloc[rankings.argsort(descending=True).cpu()]['MovieID_index'].values][:self.top_k]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train loss: 0.6033 | Test loss: 0.6121 |\n",
      "Epoch #2 | Train loss: 0.5482 | Test loss: 0.5982 |\n",
      "Epoch #3 | Train loss: 0.5350 | Test loss: 0.5908 |\n",
      "Epoch #4 | Train loss: 0.5280 | Test loss: 0.5862 |\n",
      "Epoch #5 | Train loss: 0.5218 | Test loss: 0.5855 |\n",
      "Epoch #6 | Train loss: 0.5024 | Test loss: 0.5850 |\n",
      "Epoch #7 | Train loss: 0.4974 | Test loss: 0.5852 |\n",
      "Epoch #8 | Train loss: 0.4940 | Test loss: 0.5866 |\n",
      "Epoch #9 | Train loss: 0.4909 | Test loss: 0.5888 |\n",
      "Epoch #10 | Train loss: 0.4878 | Test loss: 0.5915 |\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLPModel(num_embeddings, trainloader, testloader, device=device)\n",
    "model = mlp_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    \"\"\"Implementation of Factorization Machine acccording to \"Factorization \n",
    "    Machines\" by Rendle.\n",
    "    https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, k):\n",
    "        \"\"\"Constructor of FMM class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            size of feature vector\n",
    "        \n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Embedding(n, 1)\n",
    "        self.embeddings = nn.Embedding(n, k)\n",
    "        self._init_trunc_normal()\n",
    "    \n",
    "    def _init_trunc_normal(self, mean=0., std=0.01):\n",
    "        \"\"\"Initialize weights via truncated normal function.\n",
    "        \n",
    "        Implemented according to \"An Exploration of Word Embedding \n",
    "        Initialization in Deep-Learning Tasks\" by Kocmi T. and Bojar O.\n",
    "        https://arxiv.org/pdf/1711.09160.pdf\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : float (default: 0.00)\n",
    "            mean of normal distribution\n",
    "        \n",
    "        std : float (default: 0.01)\n",
    "            standard deviation of normal distribution\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): \n",
    "            self.embeddings.weight.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "            self.bias.weight.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"Compute interactions using Lemma 3.1\"\n",
    "        bias = self.bias(x).squeeze().sum(1)\n",
    "        embeded = self.embeddings(x)\n",
    "        pow_of_sum = embeded.sum(dim=1).pow(2)\n",
    "        sum_of_pow = embeded.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum - sum_of_pow).sum(1) * 0.5\n",
    "        y = torch.sigmoid(self.w0 + bias + pairwise)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel: \n",
    "    \n",
    "    def __init__(self, n, k, trainloader, testloader, \n",
    "                 lr=1e-3, weight_decay=1e-5, epochs=10, device=\"cuda\", top_k=50):\n",
    "        \"\"\"Constructor of training routine.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            size of feature vector\n",
    "        \n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        trainloader : torch.utils.data.DataLoader \n",
    "            iterator over training data\n",
    "        \n",
    "        testloader : torch.utils.data.DataLoader \n",
    "            iterator over test data\n",
    "        \n",
    "        lr : float\n",
    "            learning rate for optimizer\n",
    "        \n",
    "        weight_decay : float\n",
    "            regularization parameter for optimizer\n",
    "        \n",
    "        epochs : int\n",
    "            number of training epochs\n",
    "        \n",
    "        device : str\n",
    "            device to use for training\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.model = FactorizationMachine(n, k).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, \n",
    "                                                        milestones=list(range(0, epochs + 1, 5))[1:], \n",
    "                                                        gamma=0.1)\n",
    "        self.criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "        \"High-level outline of the training process.\"\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_loss = self._test()\n",
    "            self.scheduler.step()\n",
    "            if not verbose:\n",
    "                print(f\"Epoch #{epoch + 1} | Train loss: {train_loss:.4f} | Test loss: {valid_loss:.4f} |\")\n",
    "        if verbose:\n",
    "            print(f\"Final train loss: {train_loss:.4f}, test loss: {valid_loss:.4f}\")\n",
    "        return self.model\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        \"Train model for one epoch on train dataset.\"\n",
    "        device = self.device\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(x.to(device))\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.trainloader.dataset)\n",
    "    \n",
    "    def _test(self):\n",
    "        \"Test model on validation dataset.\"\n",
    "        device = self.device\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        for x, y in self.testloader:                    \n",
    "            with torch.no_grad():\n",
    "                y_hat = self.model(x.to(device))\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            test_loss += loss.item() * x.shape[0]\n",
    "        return test_loss / len(self.testloader.dataset)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        user_id = int(x.UserID_index.unique())\n",
    "        age = int(x.Age_index.unique())\n",
    "        gender = int(x.Gender_index.unique())\n",
    "        occupation = int(x.Occupation_index.unique())\n",
    "\n",
    "        movies = movielens_df_indexed.drop_duplicates('MovieID_index').copy()\n",
    "        movie_embeddings = model.embeddings(torch.tensor(movies['MovieID_index'].values,device=device).long())\n",
    "        movie_biases = model.bias(torch.tensor(movies['MovieID_index'].values,device=device).long())\n",
    "\n",
    "        user_embedding = model.embeddings(torch.tensor(user_id,device=device))\n",
    "        age_embedding = model.embeddings(torch.tensor(age,device=device))\n",
    "        gender_embedding = model.embeddings(torch.tensor(gender,device=device))\n",
    "        occupation_embedding = model.embeddings(torch.tensor(occupation,device=device))\n",
    "\n",
    "        metadata_embedding = user_embedding + age_embedding + gender_embedding + occupation_embedding\n",
    "        rankings = movie_biases.squeeze() + (metadata_embedding * movie_embeddings).sum(1)\n",
    "        recommendations = [i for i in movies.iloc[rankings.argsort(descending=True).cpu()]['MovieID_index'].values][:self.top_k]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train loss: 0.5659 | Test loss: 0.5850 |\n",
      "Epoch #2 | Train loss: 0.5387 | Test loss: 0.5823 |\n",
      "Epoch #3 | Train loss: 0.5326 | Test loss: 0.5818 |\n",
      "Epoch #4 | Train loss: 0.5275 | Test loss: 0.5815 |\n",
      "Epoch #5 | Train loss: 0.5224 | Test loss: 0.5812 |\n",
      "Epoch #6 | Train loss: 0.5093 | Test loss: 0.5809 |\n",
      "Epoch #7 | Train loss: 0.5072 | Test loss: 0.5809 |\n",
      "Epoch #8 | Train loss: 0.5060 | Test loss: 0.5810 |\n",
      "Epoch #9 | Train loss: 0.5050 | Test loss: 0.5810 |\n",
      "Epoch #10 | Train loss: 0.5041 | Test loss: 0.5813 |\n"
     ]
    }
   ],
   "source": [
    "factorization_machine = FactorizationMachineModel(num_embeddings, 120, trainloader, testloader, device=device)\n",
    "model = factorization_machine.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assess Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Item Popularity model is 74.11%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, item_popularity_model)\n",
    "print(f\"Accuracy of Item Popularity model is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Multilayer Perceptron is 85.41%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, mlp_model)\n",
    "print(f\"Accuracy of Multilayer Perceptron is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Factorization Machine is 89.31%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, factorization_machine)\n",
    "print(f\"Accuracy of Factorization Machine is {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQn80GKbpYhOi2XJA3cST3",
   "collapsed_sections": [],
   "name": "modeling.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "703acd46c16bd94cd5beac13ad6ec045c1e20ddb4c1ab0d31094b5ca090ec4cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
