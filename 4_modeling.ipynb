{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommender System\n",
    "**Course:** Data Mining <br>\n",
    "**Authors:** Lada Morozova, Shamil Arslanov, Danis Alukaev, Maxim Faleev, Rizvan Iskaliev <br>\n",
    "**Group:** B19-DS-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Modeling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes modelling tecniques employed in this project. We have organised our development process in iterative manner, i.e. on each iteration we came up with a novel architecture that potentially outperforms the current SoA method. Recent advances in the field were taken from [\"A review on deep learning for recommender systems: challenges and remedies\"](https://link.springer.com/article/10.1007/s10462-018-9654-y) by Batmaz Z. et al. In total, there are four different implemented models: item popularity model, multilayer perceptron, factorization machine, and behaviour sequence transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Item Popularity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model grounds on assumption that most popular movies are likely to be interested for a user. We define popularity in terms of ratings number. Thus, for each user recommender system always return top-k movies with a highest number of ratings. Certainly, this is a naive approach and is not likely to satisfy our data mining goal. For this reason, we will assume that the company uses this model in its current setup, and benchmarking of all other (more sophisticated) models will be performed in comparison with performance of this \"baseline\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model is simple, the great thing about it is the lack of assumptions about the data. Essentially, we can apply this method to any dataset until it contains user ratings and names of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quite natural option is to use Multilayer Perceptron (MLP). As an input this model takes concatenated latent representation of user, movie, age, occupation, gender, as well as other manually derived on previous step features. This data is passed through multiple dense layers with non-linear activations, e.g. rectified linear unit. The output layer is a single neuron with logit, that is activated using sigmoid function. The result can be interpreted as a probability that the user will enjoy the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model uses PyTorch embeddings to encode user, movie, age, occupation and gender. Therefore, the input tensor should consist of indices that can be refered to these embeddings. Also, it is quite important that the input tensor is uniformly distributed, no missing values are allowed, and the input is numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until recently, de facto the silver bullet in the field of recommendation systems were factorization machines. They were firstly presented in [\"Factorization Machines\"](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) by Rendel S. in 2010. It was inspired by and proposed as a substitution for traditional matrix factorization. This approach allows to learn interactions between user and movies, and at the same time involve user metadata for final prediction. Thus, this approach resolves well-known problem of the [cold-start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model uses PyTorch embeddings to encode user, movie, age, occupation and gender. Therefore, the input tensor should consist of indices that can be refered to these embeddings. Also, it is quite important that the input tensor is uniformly distributed, no missing values are allowed, and the input is numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we decided to apply Behaviour Sequence Transformer (BST) architecture for our problem. The implementation is mainly inspired by manuscript [\"Behaviour Sequence Transformer for E-commerce Recommendation in Alibaba\"](https://arxiv.org/pdf/1905.06874.pdf). Long story short, authors expand MLP using sequence-to-sequence model with multi-head attention mechanism. Now the model not only takes into account metadata of user and movie, but also rating history. They claim to increase online Click-Through-Rate (CTR) gain by 7.57% compared to a control group using BST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*ThXfG04qnukM6fqWLRb_JQ.jpeg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developed model uses PyTorch embeddings to encode user, movie, age, occupation and gender. Therefore, the input tensor should consist of indices that can be refered to these embeddings. Also, it is quite important that the input tensor is uniformly distributed, no missing values are allowed, and the input is numeric. Moreover, the input sequence must have an upper limit, it order to feed the behaviour sequence in model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Test Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to slightly relax our problem, let's reformulate it from regression to binary classification. Recall that in original dataset grades are from 1 to 5, thus new target is 1 if the grade above 3, and 0 - otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be splitted in train, test, and validations subsets with size of approximately 70, 20, and 10% respectively. The data is splitted according to the timestamp to make sure that it inherits its historical order. Iteratively model is trained on train subset and tested on test subset (to check how model generalizes data, detect over-fitting, etc). Final performance of models is checked on validation subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankly speaking, the recommender system cannot be properly tested without external validation. Apparently, it seems that the best way is to design the A/B testing, and check how the new approach influenced certain metric, e.g. CTR. This project does not require deployment, so building infrastructure is out of the interest scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_dir = Path(\"./data/movie_lens/movie_lens_1m.csv\")\n",
    "joint_dir = Path(\"./data/augmented_movie_lens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df = pd.read_csv(movielens_dir)\n",
    "joint_df = pd.read_csv(joint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(df, columns):\n",
    "    _df = df.copy()\n",
    "    offset = 0\n",
    "    for column in columns:\n",
    "        index_column = column + '_index'\n",
    "        _df[index_column] = offset + _df[column].astype('category').cat.codes\n",
    "        offset += len(_df[index_column].unique())\n",
    "    return _df\n",
    "\n",
    "columns = ['UserID', 'MovieID', 'Gender', 'Age', 'Occupation']\n",
    "movielens_df_indexed = index(movielens_df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    _df = df.copy()\n",
    "    train_df = _df[df.Timestamp <= 975e6]\n",
    "    test_df = _df[(df.Timestamp > 975e6) & (df.Timestamp < 98e7)]\n",
    "    val_df = _df[df.Timestamp >= 98e7]\n",
    "    return train_df, test_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movielens_df, test_movielens_df, val_movielens_df = split_dataset(movielens_df_indexed)\n",
    "train_joint_df, test_joint_df, val_joint_df = split_dataset(joint_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(movielens_df) == len(train_movielens_df) + len(test_movielens_df) + len(val_movielens_df)\n",
    "assert len(joint_df) == len(train_joint_df) + len(test_joint_df) + len(val_joint_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_mapping = dict(movielens_df_indexed[['Title', 'MovieID_index']].drop_duplicates().values[:, ::-1])\n",
    "gender_mapping = dict(movielens_df_indexed[['Gender', 'Gender_index']].drop_duplicates().values[:, ::-1])\n",
    "age_mapping = dict(movielens_df_indexed[['Age', 'Age_index']].drop_duplicates().values[:, ::-1])\n",
    "occupation_mapping = dict(movielens_df_indexed[['Occupation', 'Occupation_index']].drop_duplicates().values[:, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['UserID_index', 'MovieID_index', 'Age_index', 'Gender_index', 'Occupation_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_target = np.where(train_movielens_df.Rating.values < 4, 0, 1)\n",
    "X = torch.LongTensor(train_movielens_df[features].values)\n",
    "y = torch.LongTensor(binary_target).float()\n",
    "trainset = data.TensorDataset(X, y)\n",
    "\n",
    "binary_target = np.where(test_movielens_df.Rating.values < 4, 0, 1)\n",
    "X = torch.LongTensor(test_movielens_df[features].values)\n",
    "y = torch.LongTensor(binary_target).float()\n",
    "testset = data.TensorDataset(X, y)\n",
    "\n",
    "binary_target = np.where(val_movielens_df.Rating.values < 4, 0, 1)\n",
    "X = torch.LongTensor(val_movielens_df[features].values)\n",
    "y = torch.LongTensor(binary_target).float()\n",
    "valset = data.TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(trainset, batch_size=1024, shuffle=True)\n",
    "testloader = data.DataLoader(testset, batch_size=1024, shuffle=True)\n",
    "valloader = data.DataLoader(valset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = movielens_df_indexed[features].values.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metric Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of model is defined in terms of percentage of suggested movies which the user rated 4 or 5 among suggested movies which user rated (further refered as accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(ratings, recommendation):\n",
    "    \"\"\"Compute accuracy of recommender system.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ratings : pandas.DataFrame\n",
    "        dataframe with user's ratings\n",
    "    \n",
    "    recommendation : iterable \n",
    "        recommendations of a system\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        percentage of suggested movies which the user rated 4 or 5 \n",
    "        among suggested movies which user rated\n",
    "    \"\"\"\n",
    "    well_rated = ratings[ratings.Rating >= 4].MovieID_index\n",
    "    all_rated = ratings.MovieID_index\n",
    "    hits = len(list(set(well_rated) & set(recommendation)))\n",
    "    intersection = len(list(set(all_rated) & set(recommendation)))\n",
    "    if intersection == 0:\n",
    "        return np.NaN\n",
    "    accuracy = hits / intersection\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_accuracy(df, model):\n",
    "    \"\"\"Compute mean accuracy of inferenced model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        validation dataset\n",
    "\n",
    "    model : Object or torch.nn.Module\n",
    "        instance of class implementing predict method\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_accuracy : float\n",
    "        mean accuracy of recommendations\n",
    "    \"\"\"\n",
    "    accuracies = list()\n",
    "    for user_id in df.UserID.unique():\n",
    "        ratings = df[df.UserID == user_id]\n",
    "        indices = model.predict(ratings)\n",
    "        ratings = ratings[['MovieID_index', 'Rating']]\n",
    "        accuracy = compute_accuracy(ratings, indices)\n",
    "        accuracies.append(accuracy)\n",
    "    accuracies = np.array(accuracies)\n",
    "    accuracies = accuracies[~np.isnan(accuracies)]\n",
    "    mean_accuracy = accuracies.mean()\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains implementations for item popularity model, multilayer perceptron, factorization machine, and transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Item Popularity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPopularityModel:\n",
    "    \"\"\"Implementation of Item Popularity model for recommender systems.\"\"\"\n",
    "\n",
    "    def __init__(self, top_k=50):\n",
    "        \"\"\"Constructor of Item Popularity model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        top_k : int\n",
    "            number of items to recommend\n",
    "        \"\"\"\n",
    "        self.top_k = top_k\n",
    "        self.prediction = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Training routine.\n",
    "\n",
    "        Sorts movies by nubmer of ratings. Sets indices of top k movies in \n",
    "        attribute prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            data to use for training\n",
    "        \"\"\"\n",
    "        assert 'MovieID_index' in df and 'Rating' in df, '\"MovieID_index\" and \"Rating\" should be in column names'\n",
    "        _df = df.copy()\n",
    "        ratings_number = _df[['MovieID_index', 'Rating']].groupby(['MovieID_index']).count()\n",
    "        sorted_titles = ratings_number.sort_values(by='Rating', ascending=False)\n",
    "        sorted_titles.reset_index(inplace=True)\n",
    "        self.prediction = sorted_titles\n",
    "        \n",
    "    def predict(self, df=None):\n",
    "        \"\"\"Inference model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            data to use for training\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        indices : list\n",
    "            indices of recommended movies\n",
    "        \"\"\"\n",
    "        recommendation = self.prediction[:self.top_k]\n",
    "        indices = recommendation['MovieID_index']\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_popularity_model = ItemPopularityModel()\n",
    "item_popularity_model.fit(train_movielens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Implementation of Multilayer Perceptron with embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, n, layers=[600, 400, 200, 50]):\n",
    "        \"\"\"Constructor of class MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            number of entities to embed\n",
    "        \n",
    "        layers : list\n",
    "            list of layers sizes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert layers[0] % 5 == 0\n",
    "        self.embeddings = nn.Embedding(n, layers[0] // 5)\n",
    "        architecture = list()\n",
    "        for idx in range(len(layers) - 1):\n",
    "            architecture.append(nn.Linear(layers[idx], layers[idx + 1]))\n",
    "            architecture.append(nn.ReLU())\n",
    "        architecture.append(nn.Linear(layers[-1], 1))\n",
    "        architecture.append(nn.Sigmoid())\n",
    "        self.main = nn.Sequential(*architecture)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeded = self.embeddings(x)\n",
    "        x = embeded.flatten(start_dim=1)\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel: \n",
    "    \n",
    "    def __init__(self, n, trainloader, testloader, layers=[600, 400, 200, 50],\n",
    "                 lr=1e-3, weight_decay=1e-5, epochs=10, top_k=50, optimizer=None,\n",
    "                 device=\"cuda\"):\n",
    "        self.n = n\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.model = MLP(n, layers).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, \n",
    "                                                        milestones=list(range(0, epochs + 1, 5))[1:], \n",
    "                                                        gamma=0.1)\n",
    "        self.criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "        \"High-level outline of the training process.\"\n",
    "        min_val_loss = 1e9\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_loss = self._test()\n",
    "            self.scheduler.step()\n",
    "            if not verbose:\n",
    "                print(f\"Epoch #{epoch + 1} | Train loss: {train_loss:.4f} | Test loss: {valid_loss:.4f} |\")\n",
    "            if valid_loss < min_val_loss:\n",
    "                torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'loss': valid_loss, \n",
    "                            }, \n",
    "                            os.path.join(\"./weights/mlp/\", f\"{epoch}.pt\")\n",
    "                )\n",
    "                min_val_loss = valid_loss\n",
    "        return train_loss, valid_loss\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        \"Train model for one epoch on train dataset.\"\n",
    "        device = self.device\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(x.to(device)).flatten()\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.trainloader.dataset)\n",
    "    \n",
    "    def _test(self):\n",
    "        \"Test model on validation dataset.\"\n",
    "        device = self.device\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        for x, y in self.testloader:                    \n",
    "            with torch.no_grad():\n",
    "                y_hat = self.model(x.to(device)).flatten()\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            test_loss += loss.item() * x.shape[0]\n",
    "        return test_loss / len(self.testloader.dataset)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        user_id = int(x.UserID_index.unique())\n",
    "        age = int(x.Age_index.unique())\n",
    "        gender = int(x.Gender_index.unique())\n",
    "        occupation = int(x.Occupation_index.unique())\n",
    "\n",
    "        movies = movielens_df_indexed.drop_duplicates('MovieID_index').copy()\n",
    "\n",
    "        rankings = []\n",
    "        for movie in movies.MovieID_index.unique():\n",
    "            ranking = self.model(torch.LongTensor([user_id, movie, age, gender, occupation]).unsqueeze(0).to(device))\n",
    "            rankings.append(ranking.item())\n",
    "        \n",
    "        rankings = torch.FloatTensor(rankings)\n",
    "        recommendations = [i for i in movies.iloc[rankings.argsort(descending=True).cpu()]['MovieID_index'].values][:self.top_k]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_mlp_model(trial):\n",
    "    \"\"\"Optuna auxiliary method.\"\"\"\n",
    "    input_dim = trial.suggest_int(name=\"input_dim\", low=50, high=1000, step=5)\n",
    "    hidden_dim = trial.suggest_int(name=\"hidden_dim\", low=50, high=1000, step=1)\n",
    "    depth_hidden = trial.suggest_int(name=\"depth\", low=1, high=10, step=1)    \n",
    "    layers = [input_dim] + [hidden_dim] * depth_hidden \n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]) \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    mlp = MLPModel(num_embeddings, trainloader, testloader, epochs=5, layers=layers, device=device)\n",
    "    optimizer = getattr(optim, optimizer_name)(mlp.model.parameters(), lr=lr)\n",
    "    mlp.optimizer = optimizer\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mlp(trial):\n",
    "    \"\"\"Optuna auxiliary method.\"\"\"\n",
    "    model = define_mlp_model(trial)\n",
    "    _, loss = model.fit(verbose=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization_mlp(n_trials=15):\n",
    "    \"\"\"Hyperparameter optimization for MLP model.\"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective_mlp, n_trials=n_trials, timeout=900)\n",
    "    trial = study.best_trial\n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-21 04:49:44,999]\u001b[0m A new study created in memory with name: no-name-27363ed1-f64b-4f57-b151-e3e71ce5e0bb\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:50:35,122]\u001b[0m Trial 0 finished with value: 0.6419955693373888 and parameters: {'input_dim': 850, 'hidden_dim': 918, 'depth': 4, 'optimizer': 'RMSprop', 'lr': 1.3659722937472789e-05}. Best is trial 0 with value: 0.6419955693373888.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:51:22,936]\u001b[0m Trial 1 finished with value: 43.27547892681892 and parameters: {'input_dim': 760, 'hidden_dim': 605, 'depth': 8, 'optimizer': 'RMSprop', 'lr': 0.0032513879083991538}. Best is trial 0 with value: 0.6419955693373888.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:52:08,389]\u001b[0m Trial 2 finished with value: 0.6022213128398723 and parameters: {'input_dim': 265, 'hidden_dim': 119, 'depth': 6, 'optimizer': 'Adam', 'lr': 0.0076556290815639715}. Best is trial 2 with value: 0.6022213128398723.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:52:46,734]\u001b[0m Trial 3 finished with value: 0.5900556705759868 and parameters: {'input_dim': 970, 'hidden_dim': 98, 'depth': 2, 'optimizer': 'Adam', 'lr': 0.011689135009430755}. Best is trial 3 with value: 0.5900556705759868.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:53:26,888]\u001b[0m Trial 4 finished with value: 0.6357213248711984 and parameters: {'input_dim': 315, 'hidden_dim': 257, 'depth': 4, 'optimizer': 'Adam', 'lr': 0.00013429221106099544}. Best is trial 3 with value: 0.5900556705759868.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:54:15,318]\u001b[0m Trial 5 finished with value: 0.6065926056468349 and parameters: {'input_dim': 920, 'hidden_dim': 727, 'depth': 6, 'optimizer': 'Adam', 'lr': 0.0015204759032874941}. Best is trial 3 with value: 0.5900556705759868.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:54:53,288]\u001b[0m Trial 6 finished with value: 43.27547893131857 and parameters: {'input_dim': 750, 'hidden_dim': 196, 'depth': 2, 'optimizer': 'RMSprop', 'lr': 0.004392076194974445}. Best is trial 3 with value: 0.5900556705759868.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:55:30,828]\u001b[0m Trial 7 finished with value: 0.5869564767291932 and parameters: {'input_dim': 320, 'hidden_dim': 298, 'depth': 1, 'optimizer': 'Adam', 'lr': 0.02870069564380648}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:56:08,951]\u001b[0m Trial 8 finished with value: 0.6322584636844083 and parameters: {'input_dim': 655, 'hidden_dim': 971, 'depth': 1, 'optimizer': 'Adam', 'lr': 3.9413361756082084e-05}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:56:47,072]\u001b[0m Trial 9 finished with value: 0.6842820911118408 and parameters: {'input_dim': 75, 'hidden_dim': 155, 'depth': 4, 'optimizer': 'SGD', 'lr': 0.0022856516166627028}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:57:30,912]\u001b[0m Trial 10 finished with value: 0.684443201920534 and parameters: {'input_dim': 465, 'hidden_dim': 391, 'depth': 9, 'optimizer': 'SGD', 'lr': 0.08924842127527341}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:58:07,964]\u001b[0m Trial 11 finished with value: 43.27547893364874 and parameters: {'input_dim': 500, 'hidden_dim': 397, 'depth': 1, 'optimizer': 'Adam', 'lr': 0.054111680383202174}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:58:48,175]\u001b[0m Trial 12 finished with value: 43.275478934934355 and parameters: {'input_dim': 310, 'hidden_dim': 334, 'depth': 3, 'optimizer': 'Adam', 'lr': 0.01840188187404972}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 04:59:25,896]\u001b[0m Trial 13 finished with value: 0.6317295146845205 and parameters: {'input_dim': 60, 'hidden_dim': 53, 'depth': 2, 'optimizer': 'Adam', 'lr': 0.00034027930612951646}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:00:02,349]\u001b[0m Trial 14 finished with value: 0.5993090925986938 and parameters: {'input_dim': 985, 'hidden_dim': 509, 'depth': 1, 'optimizer': 'Adam', 'lr': 0.021138114095712247}. Best is trial 7 with value: 0.5869564767291932.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'input_dim': 320, 'hidden_dim': 298, 'depth': 1, 'optimizer': 'Adam', 'lr': 0.02870069564380648}\n"
     ]
    }
   ],
   "source": [
    "params = hyperparameter_optimization_mlp()\n",
    "print(f\"Best params: {params}\")\n",
    "layers = [params['input_dim']] + [params['hidden_dim']] * params['depth']\n",
    "mlp_model = MLPModel(num_embeddings, trainloader, testloader, layers=layers, device=device)\n",
    "optimizer = getattr(optim, params['optimizer'])(mlp_model.model.parameters(), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train loss: 0.6111 | Test loss: 0.6236 |\n",
      "Epoch #2 | Train loss: 0.5526 | Test loss: 0.5999 |\n",
      "Epoch #3 | Train loss: 0.5379 | Test loss: 0.5897 |\n",
      "Epoch #4 | Train loss: 0.5308 | Test loss: 0.5864 |\n",
      "Epoch #5 | Train loss: 0.5247 | Test loss: 0.5866 |\n",
      "Epoch #6 | Train loss: 0.5102 | Test loss: 0.5827 |\n",
      "Epoch #7 | Train loss: 0.5074 | Test loss: 0.5819 |\n",
      "Epoch #8 | Train loss: 0.5058 | Test loss: 0.5818 |\n",
      "Epoch #9 | Train loss: 0.5044 | Test loss: 0.5830 |\n",
      "Epoch #10 | Train loss: 0.5031 | Test loss: 0.5821 |\n"
     ]
    }
   ],
   "source": [
    "_, _ = mlp_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embeddings): Embedding(9776, 64)\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=298, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=298, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(\"weights/mlp/\", \"7.pt\"))\n",
    "mlp_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "mlp_model.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    \"\"\"Implementation of Factorization Machine acccording to \"Factorization \n",
    "    Machines\" by Rendle.\n",
    "    https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, k):\n",
    "        \"\"\"Constructor of FMM class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            size of feature vector\n",
    "        \n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Embedding(n, 1)\n",
    "        self.embeddings = nn.Embedding(n, k)\n",
    "        self._init_trunc_normal()\n",
    "    \n",
    "    def _init_trunc_normal(self, mean=0., std=0.01):\n",
    "        \"\"\"Initialize weights via truncated normal function.\n",
    "        \n",
    "        Implemented according to \"An Exploration of Word Embedding \n",
    "        Initialization in Deep-Learning Tasks\" by Kocmi T. and Bojar O.\n",
    "        https://arxiv.org/pdf/1711.09160.pdf\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : float (default: 0.00)\n",
    "            mean of normal distribution\n",
    "        \n",
    "        std : float (default: 0.01)\n",
    "            standard deviation of normal distribution\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): \n",
    "            self.embeddings.weight.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "            self.bias.weight.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"Compute interactions using Lemma 3.1\"\n",
    "        bias = self.bias(x).squeeze().sum(1)\n",
    "        embeded = self.embeddings(x)\n",
    "        pow_of_sum = embeded.sum(dim=1).pow(2)\n",
    "        sum_of_pow = embeded.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum - sum_of_pow).sum(1) * 0.5\n",
    "        y = torch.sigmoid(self.w0 + bias + pairwise)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel: \n",
    "    \n",
    "    def __init__(self, n, k, trainloader, testloader, \n",
    "                 lr=1e-3, weight_decay=1e-5, epochs=10, device=\"cuda\", top_k=50):\n",
    "        \"\"\"Constructor of training routine.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            size of feature vector\n",
    "        \n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        trainloader : torch.utils.data.DataLoader \n",
    "            iterator over training data\n",
    "        \n",
    "        testloader : torch.utils.data.DataLoader \n",
    "            iterator over test data\n",
    "        \n",
    "        lr : float\n",
    "            learning rate for optimizer\n",
    "        \n",
    "        weight_decay : float\n",
    "            regularization parameter for optimizer\n",
    "        \n",
    "        epochs : int\n",
    "            number of training epochs\n",
    "        \n",
    "        device : str\n",
    "            device to use for training\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.model = FactorizationMachine(n, k).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, \n",
    "                                                        milestones=list(range(0, epochs + 1, 5))[1:], \n",
    "                                                        gamma=0.1)\n",
    "        self.criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "        \"High-level outline of the training process.\"\n",
    "        min_val_loss = 1e9\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_loss = self._test()\n",
    "            self.scheduler.step()\n",
    "            if not verbose:\n",
    "                print(f\"Epoch #{epoch + 1} | Train loss: {train_loss:.4f} | Test loss: {valid_loss:.4f} |\")\n",
    "            if valid_loss < min_val_loss:\n",
    "                torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'loss': valid_loss, \n",
    "                            }, \n",
    "                            os.path.join(\"./weights/fm/\", f\"{epoch}.pt\")\n",
    "                )\n",
    "                min_val_loss = valid_loss\n",
    "        return train_loss, valid_loss\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        \"Train model for one epoch on train dataset.\"\n",
    "        device = self.device\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(x.to(device))\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.trainloader.dataset)\n",
    "    \n",
    "    def _test(self):\n",
    "        \"Test model on validation dataset.\"\n",
    "        device = self.device\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        for x, y in self.testloader:                    \n",
    "            with torch.no_grad():\n",
    "                y_hat = self.model(x.to(device))\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            test_loss += loss.item() * x.shape[0]\n",
    "        return test_loss / len(self.testloader.dataset)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        user_id = int(x.UserID_index.unique())\n",
    "        age = int(x.Age_index.unique())\n",
    "        gender = int(x.Gender_index.unique())\n",
    "        occupation = int(x.Occupation_index.unique())\n",
    "\n",
    "        movies = movielens_df_indexed.drop_duplicates('MovieID_index').copy()\n",
    "        movie_embeddings = self.model.embeddings(torch.tensor(movies['MovieID_index'].values,device=device).long())\n",
    "        movie_biases = self.model.bias(torch.tensor(movies['MovieID_index'].values,device=device).long())\n",
    "\n",
    "        user_embedding = self.model.embeddings(torch.tensor(user_id,device=device))\n",
    "        age_embedding = self.model.embeddings(torch.tensor(age,device=device))\n",
    "        gender_embedding = self.model.embeddings(torch.tensor(gender,device=device))\n",
    "        occupation_embedding = self.model.embeddings(torch.tensor(occupation,device=device))\n",
    "\n",
    "        metadata_embedding = user_embedding + age_embedding + gender_embedding + occupation_embedding\n",
    "        rankings = movie_biases.squeeze() + (metadata_embedding * movie_embeddings).sum(1)\n",
    "        recommendations = [i for i in movies.iloc[rankings.argsort(descending=True).cpu()]['MovieID_index'].values][:self.top_k]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_fm_model(trial):\n",
    "    \"\"\"Optuna auxiliary method.\"\"\"\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]) \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    embed_dim = trial.suggest_int(name=\"embed_dim\", low=10, high=1000, step=1)\n",
    "    fm = FactorizationMachineModel(num_embeddings, embed_dim, trainloader, testloader, epochs=5, device=device)\n",
    "    optimizer = getattr(optim, optimizer_name)(fm.model.parameters(), lr=lr)\n",
    "    fm.optimizer = optimizer\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fm(trial):\n",
    "    \"\"\"Optuna auxiliary method.\"\"\"\n",
    "    model = define_fm_model(trial)\n",
    "    _, loss = model.fit(verbose=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization_fm(n_trials=15):\n",
    "    \"\"\"Hyperparameter optimization for Factorization Machine.\"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective_fm, n_trials=n_trials, timeout=900)\n",
    "    trial = study.best_trial\n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-21 05:02:55,880]\u001b[0m A new study created in memory with name: no-name-371c4d77-2173-4fb5-a45c-d0c1077eb316\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:03:34,299]\u001b[0m Trial 0 finished with value: 41.08917681138435 and parameters: {'optimizer': 'Adam', 'lr': 0.07066772899461042, 'embed_dim': 601}. Best is trial 0 with value: 41.08917681138435.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:04:10,971]\u001b[0m Trial 1 finished with value: 0.6807910058580265 and parameters: {'optimizer': 'SGD', 'lr': 0.02333246853259571, 'embed_dim': 354}. Best is trial 1 with value: 0.6807910058580265.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:04:49,580]\u001b[0m Trial 2 finished with value: 0.5965228174935786 and parameters: {'optimizer': 'Adam', 'lr': 2.0152501255926844e-05, 'embed_dim': 764}. Best is trial 2 with value: 0.5965228174935786.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:05:26,474]\u001b[0m Trial 3 finished with value: 0.58588119272917 and parameters: {'optimizer': 'RMSprop', 'lr': 8.019859129538426e-05, 'embed_dim': 109}. Best is trial 3 with value: 0.58588119272917.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:06:03,370]\u001b[0m Trial 4 finished with value: 0.5828944158908625 and parameters: {'optimizer': 'RMSprop', 'lr': 0.000179503916144821, 'embed_dim': 575}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:06:40,170]\u001b[0m Trial 5 finished with value: 0.6798950559819156 and parameters: {'optimizer': 'SGD', 'lr': 0.028236627438276603, 'embed_dim': 783}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:07:17,872]\u001b[0m Trial 6 finished with value: 0.6908612209675058 and parameters: {'optimizer': 'SGD', 'lr': 4.9865984560499593e-05, 'embed_dim': 412}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:07:55,197]\u001b[0m Trial 7 finished with value: 2.6877164947464447 and parameters: {'optimizer': 'Adam', 'lr': 0.03877629543609584, 'embed_dim': 88}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:08:33,169]\u001b[0m Trial 8 finished with value: 0.6039849113141706 and parameters: {'optimizer': 'RMSprop', 'lr': 1.8227119971260276e-05, 'embed_dim': 735}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:09:11,005]\u001b[0m Trial 9 finished with value: 0.7871184202871713 and parameters: {'optimizer': 'Adam', 'lr': 0.009219234848790229, 'embed_dim': 111}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:09:47,890]\u001b[0m Trial 10 finished with value: 0.6344386782504661 and parameters: {'optimizer': 'RMSprop', 'lr': 0.0005737063433357313, 'embed_dim': 963}. Best is trial 4 with value: 0.5828944158908625.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:10:24,185]\u001b[0m Trial 11 finished with value: 0.5812858893546614 and parameters: {'optimizer': 'RMSprop', 'lr': 0.00020153938430693123, 'embed_dim': 280}. Best is trial 11 with value: 0.5812858893546614.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:11:01,598]\u001b[0m Trial 12 finished with value: 0.597503196601688 and parameters: {'optimizer': 'RMSprop', 'lr': 0.0005954850685294655, 'embed_dim': 305}. Best is trial 11 with value: 0.5812858893546614.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:11:39,482]\u001b[0m Trial 13 finished with value: 0.5832233663548203 and parameters: {'optimizer': 'RMSprop', 'lr': 0.00022351985833380015, 'embed_dim': 566}. Best is trial 11 with value: 0.5812858893546614.\u001b[0m\n",
      "\u001b[32m[I 2022-05-21 05:12:17,934]\u001b[0m Trial 14 finished with value: 0.7651455163418343 and parameters: {'optimizer': 'RMSprop', 'lr': 0.0036904935470838986, 'embed_dim': 242}. Best is trial 11 with value: 0.5812858893546614.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'optimizer': 'RMSprop', 'lr': 0.00020153938430693123, 'embed_dim': 280}\n"
     ]
    }
   ],
   "source": [
    "params = hyperparameter_optimization_fm()\n",
    "print(f\"Best params: {params}\")\n",
    "factorization_machine = FactorizationMachineModel(num_embeddings, params['embed_dim'], trainloader, testloader, device=device)\n",
    "optimizer = getattr(optim, params['optimizer'])(factorization_machine.model.parameters(), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train loss: 0.5626 | Test loss: 0.5842 |\n",
      "Epoch #2 | Train loss: 0.5369 | Test loss: 0.5835 |\n",
      "Epoch #3 | Train loss: 0.5272 | Test loss: 0.5813 |\n",
      "Epoch #4 | Train loss: 0.5173 | Test loss: 0.5819 |\n",
      "Epoch #5 | Train loss: 0.5073 | Test loss: 0.5808 |\n",
      "Epoch #6 | Train loss: 0.4867 | Test loss: 0.5806 |\n",
      "Epoch #7 | Train loss: 0.4832 | Test loss: 0.5810 |\n",
      "Epoch #8 | Train loss: 0.4810 | Test loss: 0.5816 |\n",
      "Epoch #9 | Train loss: 0.4794 | Test loss: 0.5820 |\n",
      "Epoch #10 | Train loss: 0.4779 | Test loss: 0.5826 |\n"
     ]
    }
   ],
   "source": [
    "_, _ = factorization_machine.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizationMachine(\n",
       "  (bias): Embedding(9776, 1)\n",
       "  (embeddings): Embedding(9776, 280)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(\"weights/fm/\", \"5.pt\"))\n",
    "factorization_machine.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "factorization_machine.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Sequential Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 10\n",
    "last_n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df['Rating'] = np.where(movielens_df.Rating.values < 4, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_df['valid'] = False\n",
    "last_ratings = movielens_df.groupby('UserID').filter(lambda x: len(x) >= last_n).sort_values('Timestamp').groupby('UserID').tail(last_n).sort_values('UserID')\n",
    "movielens_df.loc[last_ratings.index, 'valid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ratings = movielens_df.sort_values(by='Timestamp').groupby('UserID').agg(tuple).reset_index()\n",
    "seq_ratings['NumRatings'] = seq_ratings['Rating'].apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq(value, window_len):\n",
    "    sequences = list()\n",
    "    for idx in range(len(value)):\n",
    "        seq = value[:idx + 1]\n",
    "        if len(seq) > window_len:\n",
    "            seq = seq[idx - window_len + 1: idx + 1]\n",
    "        elif len(seq) < window_len:\n",
    "            seq = [*(['[PAD]'] * (window_len - len(seq))), *seq]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "for column in ['Title', 'Rating', 'Timestamp', 'valid']:\n",
    "    seq_ratings[column] = seq_ratings[column].apply(lambda x: make_seq(x, window_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_ratings = seq_ratings[['UserID', 'Title']].explode('Title', ignore_index=True)\n",
    "dfs = [seq_ratings[[col]].explode(col, ignore_index=True) for col in ['Rating', 'Timestamp', 'valid']]\n",
    "seq_df = pd.concat([exploded_ratings, *dfs], axis=1)\n",
    "seq_df['valid'] = seq_df['valid'].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df['TargetRating'] = seq_df['Rating'].apply(lambda x: x[-1])\n",
    "seq_df['PrevRatings'] = seq_df['Rating'].apply(lambda x: x[:-1])\n",
    "seq_df.drop(columns=['Rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df['PAD'] = seq_df['Title'].apply(lambda x: (np.array(x) == '[PAD]'))\n",
    "seq_df['NumPAD'] = seq_df['PAD'].apply(sum)\n",
    "seq_df['PAD'] = seq_df['PAD'].apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup = {str(v): i+1 for i, v in enumerate(movielens_df['UserID'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_lookup(df, feature):\n",
    "    lookup = {v: i+1 for i, v in enumerate(df[feature].unique())}\n",
    "    lookup['[PAD]'] = 0\n",
    "    return lookup\n",
    "\n",
    "movie_lookup = create_feature_lookup(movielens_df, 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "movielens_df['GenderEncoded'] = le.fit_transform(movielens_df.Gender)\n",
    "movielens_df['AgeEncoded'] = le.fit_transform(movielens_df.Age)\n",
    "movielens_df[\"UserID\"] = movielens_df[\"UserID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['UserID', 'GenderEncoded', 'AgeEncoded', 'Occupation']\n",
    "movielens_df.UserID = movielens_df.UserID.astype(int)\n",
    "seq_with_user_features = pd.merge(seq_df, movielens_df[feats].drop_duplicates(), on='UserID')\n",
    "seq_with_user_features.UserID = seq_with_user_features.UserID.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = seq_with_user_features[seq_with_user_features.valid == False]\n",
    "valid_df = seq_with_user_features[seq_with_user_features.valid == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieSequenceDataset(Dataset):\n",
    "    def __init__(self, df, movie_lookup, user_lookup):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.movie_lookup = movie_lookup\n",
    "        self.user_lookup = user_lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.df.iloc[index]\n",
    "        user_id = self.user_lookup[str(data.UserID)]\n",
    "        movie_ids = torch.tensor([self.movie_lookup[title] for title in data.Title])\n",
    "\n",
    "        previous_ratings = torch.tensor(\n",
    "            [rating if rating != \"[PAD]\" else 0 for rating in data.PrevRatings]\n",
    "        )\n",
    "\n",
    "        attention_mask = torch.tensor(data.PAD)\n",
    "        target_rating = data.TargetRating\n",
    "        encoded_features = {\n",
    "            \"UserID\": user_id,\n",
    "            \"MovieIDs\": movie_ids,\n",
    "            \"Ratings\": previous_ratings,\n",
    "            \"Age\": data[\"AgeEncoded\"],\n",
    "            \"Gender\": data[\"GenderEncoded\"],\n",
    "            \"Occupation\": data[\"Occupation\"],\n",
    "        }\n",
    "\n",
    "        return (encoded_features, attention_mask), torch.tensor(target_rating, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieSequenceDataset(train_df, movie_lookup, user_lookup)\n",
    "valid_dataset = MovieSequenceDataset(valid_df, movie_lookup, user_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "valloader = data.DataLoader(valid_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSTransformer(nn.Module):\n",
    "    \"\"\"Implementation of Behaviour Sequence Transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_movies, n_users, window_len=10, mlp_layers=[1024, 512, 256, 1],\n",
    "                embedding_size=120, n_transformers=1):\n",
    "        \"\"\"Constructor of behaviour sequence transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_movies : int\n",
    "            number of movies\n",
    "        \n",
    "        n_users : int\n",
    "            number of users\n",
    "\n",
    "        window_len : int\n",
    "            length of sequence to consider\n",
    "        \n",
    "        mlp_layers : list\n",
    "            size of layers in mlp\n",
    "\n",
    "        embedding_size : int\n",
    "            size of embedding to use\n",
    "\n",
    "        n_transformers : int\n",
    "            number of multi-head attention layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_movies = n_movies\n",
    "        self.n_users = n_users\n",
    "        self.window_len = window_len \n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_transformers = n_transformers\n",
    "        self.__build__(n_movies, n_users, window_len, mlp_layers, embedding_size, n_transformers)\n",
    "\n",
    "    def __build__(self, n_movies, n_users, window_len, mlp_layers, embedding_size, n_transformers):\n",
    "        \"\"\"Assemble behaviour sequence transformer.\"\"\"\n",
    "        self.movies_embeddings = nn.Embedding(n_movies + 1, embedding_size, padding_idx=0)\n",
    "        self.user_embeddings = nn.Embedding(n_users + 1, embedding_size)\n",
    "        self.ratings_embeddings = nn.Embedding(3, embedding_size, padding_idx=0)\n",
    "        self.sex_embeddings = nn.Embedding(3, embedding_size)\n",
    "        self.occupation_embeddings = nn.Embedding(22, embedding_size)\n",
    "        self.age_group_embeddings = nn.Embedding(8, embedding_size)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(window_len, embedding_size)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            num_layers=n_transformers,\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size, nhead=12,\n",
    "                dropout=0.15, batch_first=True, activation=\"gelu\"\n",
    "            )\n",
    "        )\n",
    "        layers = list()\n",
    "        layers.extend([nn.Linear(embedding_size * (4 + window_len), mlp_layers[0]), \n",
    "                       nn.BatchNorm1d(1024), nn.ReLU()])\n",
    "        for layer_idx in range(0, len(mlp_layers) - 1):\n",
    "            in_dim, out_dim = mlp_layers[layer_idx], mlp_layers[layer_idx + 1]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if layer_idx == len(mlp_layers) - 2:\n",
    "                break\n",
    "            layers.extend([nn.BatchNorm1d(out_dim), nn.ReLU()])\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features, mask = x\n",
    "        user_id = self.user_embeddings(features[\"UserID\"].cuda())\n",
    "        age_group = self.age_group_embeddings(features[\"Age\"].cuda())\n",
    "        sex = self.sex_embeddings(features[\"Gender\"].cuda())\n",
    "        occupation = self.occupation_embeddings(features[\"Occupation\"].cuda())\n",
    "        user_features = user_features = torch.cat((user_id, sex, age_group, occupation), 1)\n",
    "        movie_history = features[\"MovieIDs\"][:, :-1].cuda()\n",
    "        target_movie = features[\"MovieIDs\"][:, -1].cuda()\n",
    "        ratings = self.ratings_embeddings(features[\"Ratings\"].cuda())\n",
    "        encoded_movies = self.movies_embeddings(movie_history)\n",
    "        encoded_target_movie = self.movies_embeddings(target_movie)\n",
    "        positions = torch.arange(\n",
    "            0, \n",
    "            self.window_len - 1,\n",
    "            1,\n",
    "            dtype=int\n",
    "        ).cuda()\n",
    "        positions = self.position_embeddings(positions)\n",
    "        encoded_sequence_movies_with_position_and_rating = (\n",
    "            encoded_movies + ratings + positions\n",
    "        )\n",
    "        encoded_target_movie = encoded_target_movie.unsqueeze(1)\n",
    "        transformer_features = torch.cat(\n",
    "            (encoded_sequence_movies_with_position_and_rating, encoded_target_movie),\n",
    "            dim=1,\n",
    "        )\n",
    "        transformer_output = self.encoder(\n",
    "            transformer_features, src_key_padding_mask=mask.cuda()\n",
    "        )\n",
    "        transformer_output = torch.flatten(transformer_output, start_dim=1)\n",
    "        combined_output = torch.cat((transformer_output, user_features), dim=1)\n",
    "        rating = self.mlp(combined_output)\n",
    "        rating = rating.squeeze()\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSTransformerModel: \n",
    "    \n",
    "    def __init__(self, n_movies, n_users, k, trainloader, testloader, \n",
    "                 lr=1e-3, weight_decay=1e-5, epochs=10, device=\"cuda\", top_k=50):\n",
    "        \"\"\"Constructor of training routine.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_movies : int\n",
    "            number of movies\n",
    "        \n",
    "        n_users : int\n",
    "            number of users\n",
    "\n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        trainloader : torch.utils.data.DataLoader \n",
    "            iterator over training data\n",
    "        \n",
    "        testloader : torch.utils.data.DataLoader \n",
    "            iterator over test data\n",
    "        \n",
    "        lr : float\n",
    "            learning rate for optimizer\n",
    "        \n",
    "        weight_decay : float\n",
    "            regularization parameter for optimizer\n",
    "        \n",
    "        epochs : int\n",
    "            number of training epochs\n",
    "        \n",
    "        device : str\n",
    "            device to use for training\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.n_movies = n_movies\n",
    "        self.n_users = n_users\n",
    "        self.k = k\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.model = BSTransformer(n_movies, n_users, embedding_size=k).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, \n",
    "                                                        milestones=list(range(0, epochs + 1, 5))[1:], \n",
    "                                                        gamma=0.1)\n",
    "        self.criterion = nn.BCELoss().to(device)\n",
    "        \n",
    "    def fit(self, verbose=False):\n",
    "        \"High-level outline of the training process.\"\n",
    "        min_val_loss = 1e9\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_loss = self._test()\n",
    "            self.scheduler.step()\n",
    "            if not verbose:\n",
    "                print(f\"Epoch #{epoch + 1} | Train loss: {train_loss:.4f} | Test loss: {valid_loss:.4f} |\")\n",
    "            if valid_loss < min_val_loss:\n",
    "                torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'loss': valid_loss, \n",
    "                            }, \n",
    "                            os.path.join(\"./weights/bst/\", f\"{epoch}.pt\")\n",
    "                )\n",
    "                min_val_loss = valid_loss\n",
    "        return train_loss, valid_loss\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        \"Train model for one epoch on train dataset.\"\n",
    "        device = self.device\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.criterion(y_hat, y.cuda())\n",
    "            train_loss += loss.item() * len(x[0]['UserID'])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.trainloader.dataset)\n",
    "    \n",
    "    def _test(self):\n",
    "        \"Test model on validation dataset.\"\n",
    "        device = self.device\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        for x, y in self.testloader:                    \n",
    "            with torch.no_grad():\n",
    "                y_hat = self.model(x)\n",
    "            loss = self.criterion(y_hat, y.cuda())\n",
    "            test_loss += loss.item() * len(x[0]['UserID'])\n",
    "        return test_loss / len(self.testloader.dataset)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        user_id = int(x.UserID_index.unique())\n",
    "        age = int(x.Age_index.unique())\n",
    "        gender = int(x.Gender_index.unique())\n",
    "        occupation = int(x.Occupation_index.unique())\n",
    "\n",
    "        movies = movielens_df_indexed.drop_duplicates('MovieID_index').copy()\n",
    "        movie_embeddings = self.model.embeddings(torch.tensor(movies['MovieID_index'].values,device=device).long())\n",
    "        movie_biases = self.model.bias(torch.tensor(movies['MovieID_index'].values,device=device).long())\n",
    "\n",
    "        user_embedding = self.model.embeddings(torch.tensor(user_id,device=device))\n",
    "        age_embedding = self.model.embeddings(torch.tensor(age,device=device))\n",
    "        gender_embedding = self.model.embeddings(torch.tensor(gender,device=device))\n",
    "        occupation_embedding = self.model.embeddings(torch.tensor(occupation,device=device))\n",
    "\n",
    "        metadata_embedding = user_embedding + age_embedding + gender_embedding + occupation_embedding\n",
    "        rankings = movie_biases.squeeze() + (metadata_embedding * movie_embeddings).sum(1)\n",
    "        recommendations = [i for i in movies.iloc[rankings.argsort(descending=True).cpu()]['MovieID_index'].values][:self.top_k]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = BSTransformerModel(len(movie_lookup), len(user_lookup), 120, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train loss: 0.5704 | Test loss: 0.5553 |\n",
      "Epoch #2 | Train loss: 0.5471 | Test loss: 0.5384 |\n",
      "Epoch #3 | Train loss: 0.5347 | Test loss: 0.5319 |\n",
      "Epoch #4 | Train loss: 0.5272 | Test loss: 0.5347 |\n",
      "Epoch #5 | Train loss: 0.5214 | Test loss: 0.5290 |\n",
      "Epoch #6 | Train loss: 0.5022 | Test loss: 0.5258 |\n",
      "Epoch #7 | Train loss: 0.4954 | Test loss: 0.5275 |\n",
      "Epoch #8 | Train loss: 0.4899 | Test loss: 0.5267 |\n",
      "Epoch #9 | Train loss: 0.4849 | Test loss: 0.5286 |\n",
      "Epoch #10 | Train loss: 0.4795 | Test loss: 0.5278 |\n"
     ]
    }
   ],
   "source": [
    "_, _ = transformer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BSTransformer(\n",
       "  (movies_embeddings): Embedding(3708, 120, padding_idx=0)\n",
       "  (user_embeddings): Embedding(6041, 120)\n",
       "  (ratings_embeddings): Embedding(3, 120, padding_idx=0)\n",
       "  (sex_embeddings): Embedding(3, 120)\n",
       "  (occupation_embeddings): Embedding(22, 120)\n",
       "  (age_group_embeddings): Embedding(8, 120)\n",
       "  (position_embeddings): Embedding(10, 120)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=120, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.15, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=120, bias=True)\n",
       "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.15, inplace=False)\n",
       "        (dropout2): Dropout(p=0.15, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=1680, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(\"weights/bst/\", \"5.pt\"))\n",
    "transformer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "transformer.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assess Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Item Popularity model is 74.11%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, item_popularity_model)\n",
    "print(f\"Accuracy of Item Popularity model is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Multilayer Perceptron is 85.97%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, mlp_model)\n",
    "print(f\"Accuracy of Multilayer Perceptron is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Factorization Machine is 89.46%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, factorization_machine)\n",
    "print(f\"Accuracy of Factorization Machine is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Behaviour Sequence Transformer is 93.15%\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_mean_accuracy(val_movielens_df, transformer)\n",
    "print(f\"Accuracy of Behaviour Sequence Transformer is {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQn80GKbpYhOi2XJA3cST3",
   "collapsed_sections": [],
   "name": "modeling.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "703acd46c16bd94cd5beac13ad6ec045c1e20ddb4c1ab0d31094b5ca090ec4cf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
