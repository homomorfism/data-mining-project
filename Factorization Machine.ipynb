{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Modelling\n",
    "\n",
    "**Author:** Danis Alukaev <br>\n",
    "**Email:** d.alukaev@innopolis.university <br>\n",
    "\n",
    "This notebook collects my attempts to construct baseline model. I've considered well-known approach proposed by Rendle S. in [\"Factorization Machines, 2010\"](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf). In short, this model extends traditional matrix factorization by also learning interactions between different feature values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch as torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data/movielens-1m/ml-1m\")\n",
    "MOVIES_FILE_PATH = DATA_DIR / Path(\"movies.dat\")\n",
    "USERS_FILE_PATH = DATA_DIR / Path(\"users.dat\")\n",
    "RATINGS_FILE_PATH = DATA_DIR / Path(\"ratings.dat\")\n",
    "\n",
    "ENCODING = 'latin-1'\n",
    "ENGINE = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Movielens-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(MOVIES_FILE_PATH, sep='::', names=names, encoding=ENCODING, engine=ENGINE)\n",
    "movies['movie_index'] = movies['movie_id'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'gender', 'age', 'occupation', 'zipcode']\n",
    "users = pd.read_csv(USERS_FILE_PATH, sep='::', names=names, encoding=ENCODING, engine=ENGINE)\n",
    "users['user_id_index'] = users['user_id'].astype('category').cat.codes\n",
    "users['gender_index'] = users['gender'].astype('category').cat.codes\n",
    "users['age_index'] = users['age'].astype('category').cat.codes\n",
    "users['occupation_index'] = users['occupation'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'movie_id', 'rating', 'time']\n",
    "ratings = pd.read_csv(RATINGS_FILE_PATH, sep='::', names=names, encoding=ENCODING, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.join(movies.set_index('movie_id'), on='movie_id')\n",
    "ratings = ratings.join(users.set_index('user_id'), on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['user_id_index', 'movie_index', 'age_index', 'gender_index', 'occupation_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sizes = {\n",
    "    'user_id_index':len(ratings['user_id_index'].unique()),\n",
    "    'movie_index':len(ratings['movie_index'].unique()),\n",
    "    'age_index':len(ratings['age_index'].unique()),\n",
    "    'gender_index':len(ratings['gender_index'].unique()),\n",
    "    'occupation_index':len(ratings['occupation_index'].unique()),\n",
    "}\n",
    "\n",
    "next_offset = 0\n",
    "features_offsets={}\n",
    "for k,v in features_sizes.items():\n",
    "    features_offsets[k] = next_offset\n",
    "    next_offset += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in features:\n",
    "    ratings[column] = ratings[column].apply(lambda c: c + features_offsets[column])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.to_csv(DATA_DIR / \"movielens-1m.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / \"movielens-1m.csv\")[[*features, 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(df[features].values)\n",
    "y = torch.tensor(df['rating'].values).float()\n",
    "dataset = data.TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(len(dataset) * 0.9)\n",
    "test_n = len(dataset) - train_n\n",
    "splits = [train_n, test_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = torch.utils.data.random_split(dataset, splits)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Designing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    \"\"\"Implementation of Factorization Machine acccording to \"Factorization \n",
    "    Machines\" by Rendle.\n",
    "    https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, k):\n",
    "        \"\"\"Constructor of FMM class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            size of feature vector\n",
    "        \n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Embedding(n, 1)\n",
    "        self.embeddings = nn.Embedding(n, k)\n",
    "        self._init_trunc_normal()\n",
    "    \n",
    "    def _init_trunc_normal(self, mean=0., std=0.01):\n",
    "        \"\"\"Initialize weights via truncated normal function.\n",
    "        \n",
    "        Implemented according to \"An Exploration of Word Embedding \n",
    "        Initialization in Deep-Learning Tasks\" by Kocmi T. and Bojar O.\n",
    "        https://arxiv.org/pdf/1711.09160.pdf\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : float (default: 0.00)\n",
    "            mean of normal distribution\n",
    "        \n",
    "        std : float (default: 0.01)\n",
    "            standard deviation of normal distribution\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): \n",
    "            self.embeddings.weight.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "            self.bias.weight.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"Compute interactions using Lemma 3.1\"\n",
    "        bias = self.bias(x).squeeze().sum(1)\n",
    "        embeded = self.embeddings(x)\n",
    "        pow_of_sum = embeded.sum(dim=1).pow(2)\n",
    "        sum_of_pow = embeded.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum - sum_of_pow).sum(1) * 0.5\n",
    "        y = torch.sigmoid(self.w0 + bias + pairwise) * 5.5\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingRoutine: \n",
    "    \n",
    "    def __init__(self, n, k, trainloader, testloader, \n",
    "                 lr=1e-3, weight_decay=1e-5, epochs=10, device=\"cuda\"):\n",
    "        \"\"\"Constructor of training routine.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            size of feature vector\n",
    "        \n",
    "        k : int\n",
    "            size of embedding to use\n",
    "        \n",
    "        trainloader : torch.utils.data.DataLoader \n",
    "            iterator over training data\n",
    "        \n",
    "        testloader : torch.utils.data.DataLoader \n",
    "            iterator over test data\n",
    "        \n",
    "        lr : float\n",
    "            learning rate for optimizer\n",
    "        \n",
    "        weight_decay : float\n",
    "            regularization parameter for optimizer\n",
    "        \n",
    "        epochs : int\n",
    "            number of training epochs\n",
    "        \n",
    "        device : str\n",
    "            device to use for training\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        \n",
    "        self.model = FactorizationMachine(n, k).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, \n",
    "                                                        milestones=list(range(0, epochs + 1, 5))[1:], \n",
    "                                                        gamma=0.1)\n",
    "        self.criterion = nn.MSELoss().to(device)\n",
    "        \n",
    "    def fit(self):\n",
    "        \"High-level outline of the training process.\"\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_loss = self._test()\n",
    "            self.scheduler.step()\n",
    "            print(f\"Epoch #{epoch + 1} | Train loss: {(math.sqrt(train_loss)):.4f} | Test loss: {(math.sqrt(valid_loss)):.4f} |\")\n",
    "        return self.model\n",
    "    \n",
    "    def _train_one_epoch(self):\n",
    "        \"Train model for one epoch on train dataset.\"\n",
    "        device = self.device\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for x, y in self.trainloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.model(x.to(device))\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.trainloader.dataset)\n",
    "    \n",
    "    def _test(self):\n",
    "        \"Test model on validation dataset.\"\n",
    "        device = self.device\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        for x, y in self.testloader:                    \n",
    "            with torch.no_grad():\n",
    "                y_hat = self.model(x.to(device))\n",
    "            loss = self.criterion(y_hat, y.to(device))\n",
    "            test_loss += loss.item() * x.shape[0]\n",
    "        return test_loss / len(self.testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TrainingRoutine(X.max() + 1, 120, trainloader, testloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train loss: 0.9423 | Test loss: 0.9132 |\n",
      "Epoch #2 | Train loss: 0.8988 | Test loss: 0.8989 |\n",
      "Epoch #3 | Train loss: 0.8757 | Test loss: 0.8840 |\n",
      "Epoch #4 | Train loss: 0.8510 | Test loss: 0.8732 |\n",
      "Epoch #5 | Train loss: 0.8274 | Test loss: 0.8661 |\n",
      "Epoch #6 | Train loss: 0.7913 | Test loss: 0.8621 |\n",
      "Epoch #7 | Train loss: 0.7856 | Test loss: 0.8613 |\n",
      "Epoch #8 | Train loss: 0.7815 | Test loss: 0.8610 |\n",
      "Epoch #9 | Train loss: 0.7780 | Test loss: 0.8608 |\n",
      "Epoch #10 | Train loss: 0.7746 | Test loss: 0.8608 |\n"
     ]
    }
   ],
   "source": [
    "model = training.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ratings.drop_duplicates('movie_index').copy()\n",
    "movie_embeddings = model.embeddings(torch.tensor(movies['movie_index'].values,device=device).long())\n",
    "movies['embedding'] = movie_embeddings.tolist()\n",
    "movie_biases = model.bias(torch.tensor(movies['movie_index'].values,device=device).long())\n",
    "movies['bias'] = movie_biases.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Usual Suspects, The (1995)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " 'American Beauty (1999)',\n",
       " 'Godfather, The (1972)',\n",
       " 'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
       " 'Braveheart (1995)',\n",
       " 'Life Is Beautiful (La Vita Ã¨ bella) (1997)',\n",
       " 'Sanjuro (1962)',\n",
       " \"Schindler's List (1993)\",\n",
       " 'Star Wars: Episode IV - A New Hope (1977)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man_embedding = model.embeddings(torch.tensor(9754,device=device))\n",
    "age18_25_embedding = model.embeddings(torch.tensor(9747,device=device))\n",
    "metadata_embedding = man_embedding+age18_25_embedding\n",
    "rankings = movie_biases.squeeze()+(metadata_embedding*movie_embeddings).sum(1)\n",
    "[i for i in movies.iloc[rankings.argsort(descending=True).cpu()]['title'].values][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
